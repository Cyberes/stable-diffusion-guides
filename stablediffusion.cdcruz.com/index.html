<!doctype html>
	 <html lang="en">
	 	<head>


	 	<title>CDcruz| Stable Diffusion</title>
	 	<meta charset="UTF-8" />
	 	<meta name="description" content="A thorough and informative guide to Stable Diffusion and Waifu Diffusion.">
	 	<meta name="author" content="Craig D'Cruz">
	 	<meta name="viewport" content="width=device-width">
	 	<link rel='manifest' href='manifest.json'>

	 	<meta name="theme-color" content="#18BDCB"/>

		<link href="assets/css/stable_diffusion.css" rel="stylesheet">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-title" content="SD Guide">

		<meta name="apple-mobile-web-app-status-bar-style" content="black">

		<link rel="icon" type="image/x-icon" href="assets/icons/favicon.ico">
		<link rel="shortcut icon" type="image/png" href="assets/icons/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="57x57" href="assets/icons/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="assets/icons/apple-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="assets/icons/apple-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="assets/icons/apple-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="assets/icons/apple-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="assets/icons/apple-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="assets/icons/apple-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="assets/icons/apple-icon-152x152.png">
		<link rel="apple-touch-icon" sizes="180x180" href="assets/icons/apple-icon-180x180.png">
		<link rel="icon" type="image/png" sizes="192x192"  href="assets/icons/android-icon-192x192.png">
		<link rel="icon" type="image/png" sizes="32x32" href="assets/icons/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="96x96" href="assets/icons/favicon-96x96.png">
		<link rel="icon" type="image/png" sizes="16x16" href="assets/icons/favicon-16x16.png">
		<meta name="msapplication-TileColor" content="#18BDCB">
		<meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">

	 </head>
	 <script>
	 //https://css-tricks.com/add-a-service-worker-to-your-site/
	 if (navigator && navigator.serviceWorker) {
		  navigator.serviceWorker.register('sw.js');
		}
 		</script>

	<script src="assets/js/copy.js"></script>
	<script src="assets/js/load_gallery.js"></script>
	<script src="assets/js/hsection.js"></script>


  <body>




<div class='wrapper'>

	<div style="display: flex; align-items: center;">
		<img loading="lazy" src="assets/icons/original_image.webp" alt="Guide Icon" style="width: 80px; height: 80px; border-radius: 10px; margin-right: 10px;"/>
		<div>
			<h1>Stable Diffusion Guide</h1>
		  <h2>By CDcruz</h2>
		</div>
	</div>
  <hr>
  <h3 onclick="toggleHsection(this);" style="cursor:pointer; display: flex;">Contents <img loading="lazy" src="assets/images/icons/down_caret.svg" alt="down_caret" style="width: 20px; margin-left:25px;"/></h3>
  <hsection id="toc"></hsection>

  <article id="contents">
  <h3 id="intro">Introduction</h3>
  <p>This page is dedicated to storing all the tips, tricks and information that I learn about Stable Diffusion and Waifu Diffusion. I copied some parts from other authors and I will credit them where applicable/possible.</p>
  <p>As I am more interested in Waifu Diffusion and anime art styles, this guide will be mostly based around that model. However, all the information here should still apply to any kind of ai generation. I will also refer to features that are available on the webGUI by AUTOMATIC1111, which may not be available in other GUIs.</p>
	<!--<p>If you'd like to see my gallery of my personal best images you can visit my <a href="image_gallery.html">Image Gallery</a> here.</p>-->

  <h3>Getting Started</h3>
	<section>
		<p>Stable Diffusion is open source, meaning you can use it however you like and there are many methods for how to use and install Stable Diffusion. If you're more techinically minded, you could even use Stable Diffusion completely with commandline commands. There are even methods for running Stable Diffusion in the cloud, most notably Google Colab. This guide will focus on methods for running Stable Diffusion locally on your own PC that doesn't require too many steps or technical knowhow to get started.</p>

	<h4>AUTOMATIC111 WebGUI</h4>
	<textblock>
		<caution><p>Please be aware that at the time of writing the AUTOMATIC111 Github repository does not contain a license of any kind. This legally means by default all of the code in the repository is owned by the individual people who have contributed code. This means that any contributer could take down the repo with a DMCA request as their intellectual property is being used. It is an unlikely event, but a possible one & without Github's terms of service, it would not even be legal to use the WebGUI. (It may technically not be legal right now, I'm not sure, I'm not a lawyer). For more information on this, visit <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/4222" target="_blank" rel="noreferrer">Pull Request 4222</a>, <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/2059" target="_blank" rel="noreferrer">Issue 2059</a>, & <a href="https://choosealicense.com/no-permission/" target="_blank" rel="noreferrer">Information on no license</a>. Use at your own risk.</p></caution>

	<p>AUTOMATIC111's webGUI is the most popular locally run UI for stable diffusion, mostly due to the ease of installation and how quickly it is updated with new features. Following Voldy's Guide you should be able to install it without much issues.</p>
	<p>Rentry has made an excellent guide on how to get the <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noreferrer">AUTOMATIC1111 webGUI</a> to work locally on your computer. <a href="https://rentry.org/voldy" target="_blank" rel="noreferrer">Voldy's Guide</a> has a lot of good information on getting started with Stable Diffusion and Waifu Diffusion.</p>
	<p>I have created my <a href="https://stablediffusion.cdcruz.com/install.html">own guide</a> based on Voldy's that hopefully provides more information and a guide on how to install on macOS M1/M2 Macs.</p>
	<p>There are so many features available in this webGUI that i've created a seperate page to properly display features related to this GUI. <a href="https://stablediffusion.cdcruz.com/automatic1111.html">AUTOMATIC1111 WebGUI page</a>.</p>
	</textblock>
	<br>
	

	<h4>NMKD GUI - One Click Installer</h4>
	<textblock>
	<p>If following the guide above for AUTOMATIC111's webGUI is too difficult or if you're just looking to try a different GUI, a person by the name of N00MKRAD on itch.io is currently developing a standalone program to run stable diffusion that includes the core features you would want and includes dreambooth training for 24GB Nvidia GPUs as of 17/10/22, they say they are trying to reduce the VRAM requirement as well. While this tool is provided free of charge, I recommend donating to support the development of this neat program.</p>
	<p>Once you download the program, all you need to do is unzip the file on your computer and run the <tag>StableDiffusionGui.exe</tag> file. That's it!</p>
	<p>My guide will still focus more on the AUTOMATIC111 webGUI as it is usually the quickest to implement new features and has the most features to begin with, but the information should directly apply to other GUI's as they are all based on Stable Diffusion.</p>
	<p>You can download the NMKD GUI at its <a href="https://nmkd.itch.io/t2i-gui" target="_blank" rel="noreferrer">itch.io page</a>.</p>
	</textblock>

	<h4>DiffusionBee - SD for M1/M2 Macs</h4>
	<textblock>
	<p>While not as feature rich as Windows or Linux programs for Stable Diffusion, DiffusionBee is a free and open source app that brings local generation to your Mac products. It is the only MacOS program that I have currently found that installs as easy as any other app on your Mac. Simply download, open & drag to your application folder. This is by far the best installation flow of any Stable Diffusion program on any platform, it will even automatically download the Stable Diffusion model to get you started. The only downside is that is it incredibly bare bones in functionality at the current time. I am also unsure how many features are planned for this program as there is not a lot of information available on their website.</p>
	<p>Current features include, Text to Image, Image to Image and Outpainting. Those should be enough for anyone who is wanting to dabble in image generation and its ease-of-use makes up for its current lack of features.</p>
	<p>On 04/11/22 Diffusion be was updated with more features including inpainting, upscaling and improvements to all other features.</p>
	<p>Performance feels a lot slower than anticipated as well considering I am testing on an M1 Macbook Pro with 32GB RAM, it takes a considerable amount of time to render 1 image compared to my Windows PC with a Nvidia RTX 3080 with 10GB VRAM.</p>
	<h5>Requirements</h5>
	<p><tag>M1 or M2 Mac</tag></p>
	<p><tag>8GB RAM minimum - 16GB RAM recommended</tag></p>
	<p><tag>MacOS 12.5.1 or later</tag></p>
	<p>You can download DiffusionBee at its <a href="https://diffusionbee.com/" target="_blank" rel="noreferrer">official website</a>.</p>
	<br/>
	<h5>Image directory</h5>
	<p>If you're looking for where your images are saved, they are in a hidden directory at <tag>~/.diffusionbee/images/</tag>. As you cannot delete generated images within the app, you will need to navigate to this directory to delete unwanted images.</p>
	<br/>
	<h5>Models directory</h5>
	<p>The Stable Diffusion model is located at <tag>~/.diffusionbee/downloads</tag>. You can copy in your own model to this directory if you would like to use it instead of the model that downloads automatically by DiffusionBee. However, I have looked at this directory, and it seems that DiffusionBee uses <tag>.h5</tag> files instead of the standard <tag>.ckpt</tag> file.</p>
	</textblock>
	</section>

  <h3>Models</h3>
	<p>Models are what you use to generate images. Without models, you couldn't create anything. All models are <tag>.ckpt</tag> files, short for checkpoint. Most of the big models have been trained on thousands of images and the top ones like Stable Diffusion and Waifu Diffusion are continuely being trained with more epochs (one cycle of training through every image in the dataset) to create higher quality models.</p>
	<p>There are quite a few models to choose from, and more popping up all the time. Anyone with the correct computer specs and knowledge can create their own models, so as the field of AI generation continues to grow, I'm sure there will be models specialised in many genres of imagary. For now though, Stable Diffusion, Waifu Diffusion and Novel AI are king. I won't be covering Novel AI in my guide as it is a paid service and the focus of this guide is for open-source projects.</p>
	<p>For more information on some of the top models, you can visit the <a href="https://stablediffusion.cdcruz.com/models.html">models page</a>.</p>
	<section>
	<h4>Model Information</h4>
	<textblock>
	<h5>Float16 vs. Float32</h5>
	<p>As per Rentry.org. <tag>float32 for older gpus or if you want 100% precision. The outputs of both should be nearly identical, the main difference is size and the gpus that support it.</tag></p>
	<h5>Model Version</h5>
	<p>For some models, there are multiple versions of the same model available for download. There may be versions like "pruned" of "ema". Ema models are ones that are used for re-training/ continuing to train the actual model and these files will be the largest with all weights and biases and other data inside them to properly train the model. You will not need to use ema models unless you are training the model itself, they provide no extra quality to generated images. You should choose the smallest file size model when you are only wanting to generate images, as that will take up a lot less VRAM, meaning you will be able to generate more images and faster. There is minimal difference in quality between the smallest file size model and the larger ones.</p>
	</textblock>
	<h4>VAE - Variable AutoEncoders</h4>
	<textblock>
	<p>VAE's are a section of diffusion models that processes the encoding & decoding of data and helps to prevent overfitting. All model files have their own VAE already and have been using the original VAE from Stable Diffusion. Recently (around 26/10/22) new VAE's were released that are suppose to be an improvement on the original VAE.</p>
	<p>You can find these <a href="https://huggingface.co/stabilityai/sd-vae-ft-mse-original" target="_blank" rel="noreferrer">VAE files here</a>. Download the ft-MSE version for 'diffusers' models and the ft-EMA version for 'original' models. If you use the AUTOMATIC1111 WebGUI, all models are of the 'original' type.</p>
	<p>To use this VAE file, simply rename the file be the same name the .ckpt file you want to use it with but change .ckpt to .vae.pt like <tag>model.vae.pt</tag> and place it in the "models/stable-diffusion" directory of the WebGUI. This will load the VAE for the model of corresponding name.</p>
	<p>Alternatively you can also use commandline args in the webui-user.bat file and add the arg <Tag>--vae-path "path\to\your\file.vae.pt"</Tag> to use this new VAE on all models instead of just one. This method also means you can keep your VAE files anywhere you want to, it doesn't need to be in the models folder.</p>
	<p>If you have added the VAE correctly, when you relaunch the webGUI, a line similar to <tag>Loading VAE weights from: C:\stable-diffusion-webui-AUTOMATIC1111\models\Stable-diffusion\sd-v1-5-pruned-emaonly.vae.pt</tag> should appear after the main model file has loaded.</p>
	<p>For more information on VAE's Nerdy Rodent has made a good <a href="https://www.youtube.com/watch?v=O734-0SsYI4" target="_blank" rel="noreferrer">Youtube video</a> about the improved VAE versions.</p>
	<h5>More VAE's</h5>
	<p>Waifu Diffusion has a work in progress VAE available in the <a href="https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main/vae" target="_blank" rel="noreferrer">1.4 Huggingface repository</a>. As 1.4 is no where near completion, I believe this VAE is experimental but does improve anime results.</p>
	</textblock>
  </section>


  <h3>Sampling Methods</h3>
	<p>There is not a lot of information out there on what exactly sampling methods are and how they process images differently, but the results are generally consistent between methods with only minor differences. The model you use for generating images will impact the quality of an image much more than the sampling method you choose. That being said, there are subtle differences in every model that become apparent if you compare the same seed & parameters using each method. It is up to you to experiment and find what works best for you. For more information on the indiviual sampling methods, go to the <a href="https://stablediffusion.cdcruz.com/methods.html">sampling methods page</a>.</p>

  <h3>Sampling steps</h3>
	Sampling steps are how many cycles the AI will take to generate the image. While you might think intuitively that more steps will be a better image, that is not always the case. On top of this, for any sampling method that isn't an 'A' type, the image will change very minimally after roughly 300 steps. There are details added/ changed, but it is barely worth the extra processing time.</p>
	<p>For 'A' type methods, the image will change a huge amount dependant on the step size. The image will change completely if you were to render a 100 step image and a 300 step image. As with non-A methods, the detail and quality can improve with higher step counts, but it is still very minimal changes and on top of that, the image will be completely different than if you were to render the same seed at a lower step count, making it more time consuming to get the image you want.</p>
	<p>Non-A methods on the otherhand can be quickly generated at a low step count, and once you find a image you love, you can take that seed and generate it at a higher step count for better quality without a massive change in the image characteristics.</p>
	<p>To prove that extremely high sampling steps do not improve you image, I have run a small test of 20 - 5000 samplings steps so you don't have to. See the results below.</p>
	<h4>Euler - Example of a consistent (deterministic) sampling method</h4>
	<img loading="lazy" src="assets/images/examples/sampling_steps_test.webp" alt="Sampling steps example 20 to 5000 steps" style="max-width: 1200px; width: 100%;"/>
	<h4>Euler A - Example of a changing (ancestral) sampling method</h4>
	<img loading="lazy" src="assets/images/examples/sampling_steps_a_test.webp" alt="Sampling steps example 20 to 5000 steps" style="max-width: 1200px; width: 100%;"/>
	<p>The only time where a large amount of sampling steps may be useful is if you're trying to create more advanced prompts using prompt editing, alternating prompts or using embeddings that could require more processing time due to the complexity of the prompts.</p>

  <h3>Prompts Overview</h3>
	<p>Prompts are how you tell the AI what to generate. You don't need to add many prompts to generate a good image, and sometimes adding too many prompts will usually mean the AI will ignore some of them. You should try to give the AI prompts that best describe your image in the least amount of words that have the strongest affect on the model. Figuring out these strong but short prompts is the challenge of image generation and will take lots of trial and error.</p>
	<p>There are also a few helpful and important tips to know to assist in getting the image you're looking for like prompt order, parenthesis, combo words, and more that will be explained in more detail below. You can mix and match all of these tips to help refine your image as much as possible.</p>
	<p>More detailed information about prompts can be found on the <a href="https://stablediffusion.cdcruz.com/prompts.html">Prompts page</a>.</p>
	<p>A good Reddit post about prompts I've found <a href="https://www.reddit.com/r/StableDiffusion/comments/ybqn2t/prompting_what_you_need_to_know_part_1/" target="_blank" rel="noreferrer">Prompting: What you need to know (Part 1)</a>.</p>
	<p><a href="https://www.reddit.com/r/StableDiffusionInfo/comments/ylp6ep/some_detailed_notes_on_automatic1111_prompts_as/" target="_blank" rel="noreferrer">Another good post by u/StoryStoryDie</a></p>
	

	<h3>CFG Scale</h3>
	<section>
	<p>The CFG Scale (Classifier Free Guidance Scale) is a option available for txt2img and img2img. In simple terms, it lets you specify how strictly the AI should follow what you have written for your prompt. A high number means it will follow your prompt as accurately as possible while a lower number will allow the AI to loosely base its generation on your prompts and produce more varied/ random outputs.</p>
	<p>In the example below, I have generated images for a specific seed in Waifu Diffusion 1.3 for all CFG values from 0 - 32. As you can see, at 0, the AI basically generates a random mess of imagery, but as it moves away from 0, it starts to form a more coherent image. As it goes beyond 20, it starts to create more glitches and artifacts as the AI tries to conform to the prompts as strictly as possible.</p>
	<p>I have not done enough testing with this, but I assume that a higher CFG could produce good results if you provided prompts that described your image perfectly within the dataset the model has been trained on. In general though, I would keep the CFG value between 5 - 20. Of course experiment with this yourself and try the extreme values out as experimenting is what this is all about.</p>
	<img loading="lazy" src="assets/images/examples/cfg_scale_example.webp" alt="An example of the changes in CFG Scale." style="max-width: 1200px; width: 100%;"/>
	<tag>Prompts: 1girl, anime, woman AND glasses, blue hair, brown eyes, grin, beautiful face, shirt AND waistcoat, maxi skirt, frills, colorful, vibrant, deep eyes, full body, high angle, by Toei Animation, Artstation, high quality, HD, simple background</tag>
	</section>

	<h3>Image Dimensions</h3>
	<section>
	<p>As far as I know, all diffusion models have been trained on 512x512px images. This means that those dimensions are the best dimensions to use when generating your own content as the model is most familiar with that.</p>
	<p>It is very obvious when you try to generate images in different height or width than 512x512 the results can be much worse and sometimes completely unusable.</p>
	<p>There are ways to combat this and improve image quality, but the AI still struggles in most dimensions, especially landscape images.</p>

	<h4>Creating larger square images</h4>
		<textblock>
		<p>Using the AUTOMATIC1111 WebGUI, the txt2img UI has a "highres. fix" option, it isn't a perfect solution, but it can improve images of higher resolution. Together with this, you should use the "Resize seed from width/hight" options that are available in the Extra seed section. If you set the resize size to 512x512 the AI will try to create the same image it would have created at 512x512, but instead it will be created at your new resolution. It is recommended to keep the same aspect ratio for the inital size and large size, as changing the aspect ratio will change the image output.</p>
		<p>Be aware that the larger the image, the longer it will take to render and the more VRAM is required to process the image. On top of that, using the method stated above will actually generate 2 images, one at the initial 512x512 size and then another image at your higher resolution which will add even more time to your render.</p>
		</textblock>
		<h4>Creating landscape images</h4>
		<textblock>
		<p>Landscape images are the hardest of the 3 aspects to create and will generate duplicate people, conjoined people or a complete weird jumbled mess at the worst of times. It isn't a hopeless cause though, it is still possible to create a decent images or an image that is close enough that you can use inpainting to fix the issues.</p>
		<p>I personally wouldn't recommend trying to create a larger image mode than roughly 800px wide and you should keep the height at 512px so the AI has one reference point it is familiar with.</p>
		</textblock>
		<h4>Creating portrait images</h4>
		<textblock>
		<p>Portrait images are generally easier to create compared to landscape. If you're having trouble getting a full body image in the square aspect ratio, it may be helpful to try changing the size to portrait. Portrait images can still generate more glitched art than a square image, but when it does work, it goes a great job on character bodies. It is recommended to keep the image width at 512px so the AI has one size it is familiar with.</p>
		</textblock>
	</section>

	<section>
  <h3>Img2Img</h3>
	<p>Img2Img is a useful feature that allows you to take one image and apply the AI model on top of your image. You can choose to influence the original image subtly like a style filter, or intensly by changing the original image in drastic ways. One example of a drastic change would be to take an image of simple shapes and using the AI to turn it into some detailed picture.</p>
</section>

	<section>
  <h3>Inpainting</h3>
	<p>Inpainting can be a very useful tool for fixing up errors in your generated image, or to refine your image. There are a few ways to use inpainting, but the most common way is to use some sort of mask to select the area of the image you want to modify and play around with settings until you get what you're looking for.</p>
	<p>You can either draw the mask in the webGUI directly or provide a seperate mask image which would me some sort of black and white image with black being the areas you want to inpaint.</p>
	<p>A new update on 28/10/22 the WebUI added a option in the settings tab named "Inpainting conditioning mask strength" which allows you to reign back the variability of the generation to retain more qualities of the original image even with a high denoising strength setting. This is great if you would like to apply a different style to your image but don't want it to completely change the subject or composition of the image. For more information a Youtuber called Patrick Galbraith posted a good video on <a href="https://www.youtube.com/watch?v=xrJpZbZWZEo" target="_blank" rel="noreferrer">Conditioning Mask Strength</a>.</p>
	<h5>A good starting point for inpainting is to use the settings as follows:</h5>
	<textblock>
		<p>Start with the exact same prompts that you used for the original image and add/remove prompts as you see fit. If you are unsure on the original prompts, you can use the PNG Info tab to find out that infomation.</p>
		<p><tag>Mask blur = 4</tag> Keep the mask blur low, as if it's too high it will ruin parts of the image you don't want edited.</p>
		<p><tag>Masked content = original</tag> This means that the inpainting algorithm will base its editing on the original image instead of starting completely from scratch.</p>
		<p><tag>Inpaint at full resolution - unticked</tag> This will help the AI keep the style consistent with your original image. Although you can produce more detail if you tick this option, that may not look coheasive if the rest of your image is not as detailed.</p>
		<p><tag>Use "Just resize" or "Crop and resize"</tag> It shouldn't really matter which you use as long as your new image is the same dimensions as the original image.</p>
		<p><tag>Sampling method = whichever method you used for the orignal image</tag> Although, reference the Sampling Method section as you may want to use a different method for better eyes, hands, etc.</p>
		<p><tag>Image height & width = The original size.</tag></p>
		<p><tag>CFG Scale = 7</tag> Start with 7 and increase the number as you see fit. CFG Scale is how closely the model will follow the prompts you have listed, keeping the number low will produce more variety in results.</p>
		<p><tag>Denoising strength = Experiment with this</tag> This is one of the most important parameters to play with for inpainting. The lower the number, the less the inpainting will affect the image. This gives you finer control over how much your original image will change which is very useful if you only need a slight touch up on your image.</p>
		<p><tag>Seed = -1</tag> If you keep the seed as the original image seed, you won't get much change unless you change prompts. This might be what you want, but in general it would be better to leave it at random seed.</p>
	</textblock>
	<img loading="lazy" src="assets/images/examples/inpainting_example.webp" alt="Examples of inpainting." style="max-width: 1200px; width: 100%;"/>
	<p>Two examples of using the 1.5 SD inpainting model to clean up managa panels by remove text from the image.</p>
</section>


  <h3>Outpainting</h3>
	<p>Outpainting allows you to take an image and extend its borders in any direction and the AI will attempt to generate a new piece of the image from scratch. I have not played around with outpainting using the webGUI too much yet, mainly because it is quite cumbersome with the current UI and I have not been able to achieve great results. There are other UI's available that are much more flexible and user friendly, but they are also usually a paid service.</p>
	<img loading="lazy" src="assets/images/examples/outpainting_example.webp" alt="Examples of outpainting." style="max-width: 1200px; width: 100%;"/>

	<h3>X/Y Plotting</h3>
	<section>
	<p>X/Y plotting is simply a way to generate a grid of renders that combine 2 parameters in different combinations. This is more useful for researching and comparing outputs than using it for artistic purposes. It is available on the webGUI under both the txt2img tab and the img2img tab under the "script" drop-down menu options. Once selected, you'll be greeted by more options.</p>
	<p>The first section of the X/Y plot is where you choose the parameters you want to set as your X and Y axis. You can choose from a list of options for X and Y respectively and then enter comma seperated values in the corresponding inputs to specify what you want to set the x and y axis values to. Each option for an axis will have different values you can set for that option, for example, the sampler axis can only be set to a sampler name, while the seeds axis can be set to an integer number or -1 for random. If you set an axis to some invalid value, it won't even attempt to process and throw an error in the commandline console.</p>
	<p>Be aware that the more values you add, the more columns/ rows you are adding which will exponentially add images to the render. A 2x2 grid will create 4 images while a 5x5 would create 25. If your render settings are high, that can take a considerable amount of time.</p>
	<br>
	<p>There are multiple ways to enter column and row values. If you are inputting numerical values there are some formats you can use to make things easier. Examples below are taken from AUTOMATIC1111's Github page.</p>
	<p><tag>Simple ranges</tag></p>
	<p>1-5 = 1, 2, 3, 4, 5</p>
	<p><tag>Ranges with increment in bracket</tag></p>
	<p>1-5 (+2) = 1, 3, 5</p>
	<p>10-5 (-3) = 10, 7</p>
	<p>1-3 (+0.5) = 1, 1.5, 2, 2.5, 3</p>
	<p><tag>Ranges with the count in square brackets</tag></p>
	<p>1-10 [5] = 1, 3, 5, 7, 10</p>
	<p>0.0-1.0 [6] = 0.0, 0.2, 0.4, 0.6, 0.8, 1.0</p>
	<br>


	<p>The sampler option will only work if you specify the sampler names correctly. Below is a list of the names</p>
	<p><tag>euler</tag>, <tag>euler a</tag>, <tag>lms</tag>, <tag>heun</tag>, <tag>dpm2</tag>, <tag>dpm2 a</tag>, <tag>dpm fast</tag>, <tag>dpm adaptive</tag>, <tag>lms karras</tag>, <tag>dpm2 karras</tag>, <tag>dpm2 a karras</tag>, <tag>ddim</tag>, <tag>plms</tag></p>
	<p>Below is a visual example of an X/Y plot. As you can see, it generates all possible combinations of the set axis values and is a create way to compare things like samplers, seeds, step counts and many other things.</p>
	<img loading="lazy" src="assets/images/examples/xy_plotting.webp" alt="An example of X/Y plotting." style="max-width: 1200px; width: 100%;"/>
	<p>If you'd like to recreate this exact grid, see the settings below.</p>
	<p>Prompts: <tag>anime, woman, attractive, by Toei Animation, Artstation, brunette, deep eyes, [[cleavage]], kawaii, sky, vibrant, colorful, grin, clear</tag></p>
	<p>Negative Prompts: <tag>deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra_limb, ugly, poorly drawn hands, two heads,child, children, kid, gross, mutilated, disgusting, horrible, scary, evil, old, conjoined, morphed, text, error, glitch, lowres, missing digits, watermark, signature, jpeg artifacts, low quality</tag></p>
	<p>Other parameters: <tag>Model: Waifu Diffusion 1.3, Sampler: Euler a, CFG scale: 7, Seed: 1481806505, Size: 512x512, Model hash: 4470c325</tag></p>

	<h4>XYZ Plotting</h4>
	<textblock>
		<p>Similar to X/Y plotting, XYZ plotting is a very new feature just released on 17/10/22. It however, is not accessed the same way as X/Y plotting and is displayed differently since it is trying to display 3 dimensions of data instead of a 2 axis grid.</p>
	</textblock>
</section>

	<h3>Training A Model</h3>
	<p>There are several ways to currently train your own model or extend the use of a pre-existing model. Each fill affect your renders in different ways and be more effective for certain tasks you're trying to achieve. Of course if you are trying to generate images for something very niche, training a model from scratch would be your best method, however that requires a large amount of data, processing power and time so the average person would not be able to do that reasonably. Thankfully, there are smaller ways to influence the AI models without redoing everything, below are a few methods.</p>
	<p>I have not dived deep into training models due to the data and processing requirements, so my current knowledge is only from what I have learnt from other sources. Due to this, please do your own research as mine may be inaccurate and is just to give you a basic idea of the concepts.</p>
	<p>For more information visit the <a href="https://stablediffusion.cdcruz.com/model_training.html">Model Training page</a>.</p>
	<p>For a list of custom embeddings, models, hypernetworks and aesthetic gradients go to the <a href="https://stablediffusion.cdcruz.com/embeddings.html">Embeddings page</a> for more information.</p>
	

	<h3>Upscalers</h3>
	<section>
	<p>Since you will generally be creating images with dimensions 512x512, you may want to upscale your output to produce higher quality images or simply scale up to make it more useable in other programs.</p>
	<p>the AUTOMATIC1111 webGUI comes with a few good upscaling options and includes options to automatically upscale images after generating them. Although its upscalers are great, it can be a bit obtuse in how its layed out and what the best settings are for upscaling images. Due to this, I've listed some other upscalers that I know of in case you're interesting in trying them out as alternatives.</p>
	<h4>WebGUI Upscalers</h4>
	<textblock>
	<p>There are many different types of upscalers to choose from within the WebGUI. I do not know a lot about the technical details of each upscaling model but all of them produce good results. The first time you use an upscaler model it will be downloaded automatically.</p>
	<p>I have personally had trouble downloading the LSDR upscaler with an error that says <tag>[SSL: CERTIFICATE_VERIFY_FAILED]</tag>. To get around this, you can download the upscaler model manually at the link provided in the terminal which is <tag><a href="https://aaronfeng.itch.io/waifu2x-extension-gui" target="_blank" rel="noreferrer">https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1</a></tag>. Once the model is downloaded, place it in the directory <tag>/models/LDSR</tag> within your webGUI directory and rename the file to <tag>model.ckpt</tag>. The model should now work correctly.</p>
	<!--<h5>ESRGAN 4x</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-ESRGAN_4x.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>Lanczos</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-lanczos.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>LDSR</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-LDSR.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>Nearest</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-nearest.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>R-ESRGAN-General 4x3V</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-R-ESRGAN-General_4xV3.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>ScuNET</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-ScuNET.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>ScuNET PSNR</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-ScuNET_PSNR.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>
	<h5>SwinlR 4x</h5>
	<img loading="lazy" src="assets/images/examples/upscaler-LDSR.webp" alt="ESRGAN 4x Upscaler" style="max-width: 1200px; width: 100%;"/>-->
	</textblock>

	<h4>Waifu2x Extension GUI</h4>
	<textblock>
	<p>Another simple GUI software that specialises in upscaling of anime imagery, although it can work with any images. Compared to AUTOMATIC1111's webGUI, Waifu2x has a lot more features to fine tune your upscaling with different models, parameters and more technical settings. Another advantage to using Waifu2x is that it can upscale videos with the same precision as images which is a feature that I don't believe AUTOMATIC1111's webGUI has implemented yet (although knowing his pace of adding features its bound to happen eventually).</p>
	<p>You can visit Aaron Feng's <a href="https://aaronfeng.itch.io/waifu2x-extension-gui" target="_blank" rel="noreferrer">Itch.io page</a> for more information and a download link.</p>
	</textblock>
	<h4>Topaz Labs Gigapixel AI</h4>
	<textblock>
	<p>This is a paid software ($99USD at time of writing) so it doesn't fit with the open source theme of my guide, but I felt it was worth adding as it is one of the more user friendly options for more customized upscaling with multiple unique models available, multi-view rendering, batch upscaling and more. While its upscalers are more skewed to photography than artistic styles, it can still produce good results. As its a paid service (one time fee with option yearly fees), you can expect consistent updates and support using this software.</p>
	<p>You can visit Topaz Lab's <a href="https://www.topazlabs.com/gigapixel-ai" target="_blank" rel="noreferrer">official website</a> for more information.</p>
	</textblock>
	</section>

  <h3>Current model limitations and issues</h3>
	<p>Stable Diffusion and its alternatives are not perfect models. There are many limitations that all depend on the dataset they were trained on, how long they were trained and the vocabulary that is knows. Due to this, there are very obvious elements that all models stuggle with, but they do continue to improve with more training and new features.</p>
	<section>
		<h4>Hands</h4>
			<textblock>
				<p>Hands are one of the most notorious problems for the AI. There have been improvements to hands in the newest 1.3 Waifu Diffusion model, but it still has a long way to go. There are no major prompts that I have found to improve hand art. The AI will usually try to hide hands behind backs, of screen or in any other way it can to avoid drawing them. This is only a suspicion, but I believe it may be due to real artists avoiding drawing hands, and therefore, the AI would not have a large enough dataset of drawn hands to know how to do them properly. Although, Stable Diffusion also has trouble with hands in photographic imagery, but shouldn’t have the same dataset problem, as there would be plenty of images of real people's hands to train on. There is some hope though, with platforms like NovelAI achieving excellent hands and other features Stable Diffusion falls flat on. But this gives hope that Stable Diffusion can reach that same quality, eventually.</p>
			</textblock>
		<h4>Male Characters</h4>
		<textblock>
			<p>This limitation mostly relates to Waifu Diffusion, as Stable Diffusion is very good at creating male characters, however Waifu Diffusion has very obviously been trained on mainly anime women which makes male characters look quite bad compared to what the AI can generate for female characters. With WD 1.3 there have been improvements on male characters, but it could still be improved with more variety and detail for male characters.</p>
		</textblock>
		<h4>Distant Characters</h4>
		<textblock>
			<p>All models are quite good at generating characters close up, usually from the bust up, but once you try to get a full body shot or include scenery it can cause problems. The futher away a character is, the less likely they will be generated properly. It's not a solution, but the AI can generate distant characters better if they are portrayed as a silhouette as it doesn't need to create details like the face.</p>
		</textblock>
		<h4>NSFW Art</h4>
		<textblock>
			<p>Obviously NSFW art will be diffcult for most models as they usually won't be trained on a lot of NSFW data. There are a few specialized NSFW models like Hentai Diffusion, but while they are better at lewd poses, kinky imagery and BDSM art; they usually have some kind of drawback like poor facical features, backgrounds and perform even worse at details like hands, feet & eyes.</p>
			<p>You can create really good NSFW imagery with a lot of time and patience and finding the correct prompts, but results will vary a lot and the more niche your NSFW image is, the less likely it will create good results.</p>
		</textblock>
	</section>
  <h3>Credits/ Resources</h3>
  <creditblock>
		<ul>
	  <li><a href="https://huggingface.co/hakurei/waifu-diffusion" target="_blank" rel="noreferrer">https://huggingface.co/hakurei/waifu-diffusion</a></li>
	  <li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank" rel="noreferrer">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></li>
	  <li><a href="https://rentry.org/voldy" target="_blank" rel="noreferrer">https://rentry.org/voldy</a></li>
	  <li><a href="https://cyberes.github.io/stable-diffusion-models/" target="_blank" rel="noreferrer">https://cyberes.github.io/stable-diffusion-models/</a></li>
	  <li><a href="https://www.youtube.com/watch?v=c5dHIz0RyMU" target="_blank" rel="noreferrer">Youtuber: Nerdy Rodent Video: Stable Diffusion Prompts</a></li>
	  <li><a href="https://www.youtube.com/watch?v=dOsNAb-ML_o&list=PLZpDYt0cyiusIUae2YnPZ26HTjC6yuhNU" target="_blank" rel="noreferrer">Youtuber: Royal Skies Playlist: Learn Stable-Diffusion (FAST!)</a></li>
		<li><a href="https://www.reddit.com/r/StableDiffusion/comments/xz26lq/automatic1111_xformers_cross_attention_with_on/" target="_blank" rel="noreferrer">Reddit Post: AUTOMATIC111 xformers cross attention with on Windows. Author: u/Der_Doe</a></li>
		<li><a href="https://www.reddit.com/r/StableDiffusion/comments/xywtm5/new_feature_in_auto1111_alternating_prompts/" target="_blank" rel="noreferrer">Reddit Post: New feature in Auto1111 - Alternating prompts. Author: u/SnareEmu</a></li>
		<li><a href="https://wiki.installgentoo.com/wiki/Stable_Diffusion" target="_blank" rel="noreferrer">Stable Diffusion Guide by wiki.installgentoo.com</a></li>
		<li><a href="https://chara-zokusei.jp/en/question_list" target="_blank" rel="noreferrer">https://chara-zokusei.jp/en/question_list</a></li>
		<li><a href="https://www.youtube.com/watch?v=xtFFKDgyJ7A" target="_blank" rel="noreferrer">Youtuber: enigmatic_e Video: Stable Diffusion IMG2IMG settings Pt. 2 (Consistent Animations)</a></li>
		<li><a href="https://unideer.notion.site/Perspective-Research-NovelAI-9fb472f4faeb492d8e0b095d2efa0796" target="_blank" rel="noreferrer">https://unideer.notion.site/Perspective-Research-NovelAI-9fb472f4faeb492d8e0b095d2efa0796</a></li>
		<li><a href="https://www.youtube.com/watch?v=7Lxdk89W2K0" target="_blank" rel="noreferrer">Youtuber: nerdy rodent Video: Textual Inversion - Make Anything In Stable Diffusion!</a></li>
		<li><a href="https://github.com/JohannesGaessler/stable-diffusion-insights" target="_blank" rel="noreferrer">Sampling Method research by u/Remove_Ayys</a></li>
		<li><a href="https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru" target="_blank" rel="noreferrer">https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru</a></li>
		<li><a href="https://www.youtube.com/watch?v=-JtBUoPcjeM" target="_blank" rel="noreferrer">Youtuber: MattVidPro AI Video: Easy Dreambooth AI Tutorial</a></li>
		<li><a href="https://www.youtube.com/watch?v=1_dViOW3Vy4" target="_blank" rel="noreferrer">Youtuber: ChamferZone AI Video: Stable Diffusion - Master AI Art</a></li>
		<li><a href="https://www.youtube.com/watch?v=8NDz0YmSGVU" target="_blank" rel="noreferrer">Youtuber: bycloud Video: AI Generated Art Is Getting Out of Hand</a></li>
		</ul>
  </creditblock>

	<h3>Get in touch</h3>
	<p>If you'd like to reach out to me for whatever reason, feel free to contact me through any method below. I'd love to see what you create, share information about generating images or have any other Stable Diffusion related things you'd like to share with me.</p>
	<tag>Reddit: <a href="https://www.reddit.com/user/Official_CDcruz" target="_blank" rel="noreferrer">@official_cdcruz</a></tag>
	<tag>Instagram: <a href="https://www.instagram.com/official_cdcruz/" target="_blank" rel="noreferrer">@official_cdcruz</a></tag>
	<tag>Discord: CDcruz#7499</tag>
	<script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy me a waifu pillow', '#29abe0', 'A0A85XKO6');kofiwidget2.draw();</script>
  </article>


<script>

window.onload = function () {
    let contents = document.getElementById("contents").childNodes;
    document.getElementById("toc").innerHTML = create_toc(contents);
};

function create_toc(contents) {
	let toc = "";
	contents.forEach(function(item, i, arr) {
		let id = "";
		switch (item.nodeName) {
			case "H3":
				item.id = item.innerHTML;
				id = (item.id == undefined) ? "" : `#${item.id}`;
				toc += `<list1><a href="${window.location.href}/${id}">${item.innerHTML}</a></list1>`;
				break;
			case "H4":
				item.id = item.innerHTML;
				id = (item.id == undefined) ? "" : `#${item.id}`;
				toc += `<list2><a href="${window.location.href}/${id}">${item.innerHTML}</a></list2>`;
				break;
			case "SECTION":
				toc += create_toc(item.childNodes);
				break;
		}
	});
	return toc;
}

</script>

         </div>
  </body>
</html>
