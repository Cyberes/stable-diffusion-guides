<!doctype html>
	 <html lang="en">
	 	<head>


	 	<title>CDcruz| Embeddings Training</title>
	 	<meta charset="UTF-8" />
	 	<meta name="description" content="A thorough and informative guide to Stable Diffusion and Waifu Diffusion.">
	 	<meta name="author" content="Craig D'Cruz">
	 	<meta name="viewport" content="width=device-width, minimal-ui">
	 	<link rel='manifest' href='manifest.json'>

	 	<meta name="theme-color" content="#18BDCB"/>

		<link href="assets/css/stable_diffusion.css" rel="stylesheet">

		<link rel="icon" type="image/x-icon" href="assets/icons/favicon.ico">
		<link rel="shortcut icon" type="image/png" href="assets/icons/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="57x57" href="assets/icons/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="assets/icons/apple-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="assets/icons/apple-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="assets/icons/apple-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="assets/icons/apple-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="assets/icons/apple-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="assets/icons/apple-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="assets/icons/apple-icon-152x152.png">
		<link rel="apple-touch-icon" sizes="180x180" href="assets/icons/apple-icon-180x180.png">
		<link rel="icon" type="image/png" sizes="192x192"  href="assets/icons/android-icon-192x192.png">
		<link rel="icon" type="image/png" sizes="32x32" href="assets/icons/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="96x96" href="assets/icons/favicon-96x96.png">
		<link rel="icon" type="image/png" sizes="16x16" href="assets/icons/favicon-16x16.png">
		<meta name="msapplication-TileColor" content="#18BDCB">
		<meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">

	 </head>


  <body>




<div class='wrapper'>

	<h1>Textual Inversion</h1>
	<h2>Embedding Training</h2>
  <hr>
  <article>
		<a href="index.html">Back to the main page</a>
		<p>This page will provide detailed instructions on conducting your own textural inversion training to create embeddings and use them in image generation. This guide is largely based on the YouTube video <a href="https://www.youtube.com/watch?v=7Lxdk89W2K0" target="_blank" rel="noreferrer">Textual Inversion - Make Anything In Stable Diffusion!</a>. In addition, this page will include any additional findings discovered through the use of textural inversion.
			<h3>Requirements</h3>
			<p><tag>VRAM - 8GB minimum</tag></p>
			<p><tag>System RAM - 16GB minimum</tag></p>
			<p><tag>Dataset - 5 images minimum</tag></p>
			<p>For those with high-end graphics cards with significant VRAM, the following options can be enabled on the <tag>webui-user.bat</tag> file by setting the <tag>set COMMANDLINE_ARGS=</tag> to include <tag>--no-half</tag> (prevents the AI model from using 16-bit floats) and <tag>--precision full</tag> (ensures the AI uses the most accurate calculations). However, please note that my Nvidia RTX 3080 with 10GB VRAM could not run embedding training with these options enabled. If unsure, it is recommended to leave these settings off.
			<p>As a baseline for performance, my PC has a RTX 3080 with 10GB VRAM and 16GB system RAM. 1000 steps of training take approximately 45 minutes. Therefore, those with lower specs can expect longer training times, while those with better graphics cards will see improved performance.</p>

			<h3>Preparing Data</h3>
			<p>Properly preparing your data is essential for successful training of a model. In the case of textual inversion, a relatively small dataset can yield decent results. When collecting images, ensure they have a square aspect ratio, are of high quality with minimal compression or aliasing artifacts, and are in either JPEG or PNG format.</p>
			<p>The more diverse your dataset, the better your embedding will perform. Include a variety of lighting, angles, composition, and expressions in your images.</p>
			<h3>Creating the Embedding File</h3>
			<p>To train your embedding, the first step is to create the placeholder for the training data and the associated prompt. To do this, go to the "Train" tab of the webGUI and select the "Create Embedding" tab. Fill in the following fields:</p>
			<p><tag>Name</tag> This is the prompt for your embedding and will also be the filename of your embedding.</p>
			<p><tag>Initialization Text</tag> This field is for inputting a list of prompts to help the AI understand the association with your training data. There is not currently much information on the effectiveness of this field, but a good starting point is to use the Interrogator from img2img and copy the prompts it creates for one of the images from your dataset.</p>
			<p><tag>Number of Vectors per Token</tag> This refers to the size of the embedding. A larger value allows for more information to be included in the embedding, but will also decrease the number of allowed tokens in the prompt. With stable diffusion, there is a limit of 75 tokens in the prompt. For example, if you use an embedding with 16 vectors, that will leave you with space for 75 - 16 = 59 tokens. Additionally, larger numbers of vectors may require more images for good results.</p>
			<p>Once you have completed these fields, click the "Create Embedding" button to add the necessary directories and files for your embedding.</p>

			<h3>Preprocessing</h3>
			<p>Before starting the training process, it is necessary to preprocess the dataset to ensure that the AI is able to understand the images and that they are all of the correct dimensions. On the webGUI in the Train tab, navigate to the "Preprocess images" tab. From there, the following fields must be filled out:</p>
			<p><tag>Source Directory</tag> The directory path to the folder containing the training images must be provided. If unsure of how to do this, a quick internet search should provide the necessary information.</p>
			<p><tag>Destination Directory</tag> A different path to a new folder for the AI to copy the processed images must be provided.</p>
			<p><tag>Width & Height</tag> This is where the dimensions for all images are set. Unless there is a specific reason to do so, it is recommended to keep the dimensions at 512x512px.</p>
			<p><tag>Create flipped copies</tag> It is recommended to check this tickbox as it will increase the dataset size by mirroring all original images, providing the AI with twice as many images to work with. This will also improve the training as the AI will perceive the mirrored images as completely different images.</p>
			<p><tag>Split oversized images into two</tag> It is recommended to leave this tickbox unchecked. It is intended for use with images that do not have the same aspect ratio as the set width/height, but it is advised to keep all data at a square aspect ratio.</p>
			<p><tag>Use BLIP for caption or Deepdanbooru</tag> It is recommended to check this tickbox. This will add the Interrogator to the preprocessing process and include prompts associated with each image in the dataset. This will be useful in helping the AI understand the content of the images during training. Use the BLIP interrogator to generate normal sentence prompts and use Deepdanbooru to generate tag-like prompts. It is recommended to use BLIP for Stable Diffusion training and Deepdanbooru for Waifu Diffusion training.</p>
			<p>Once all settings have been configured, the "Preprocess" button can be clicked to begin the preprocessing process. This should be a quick process, but the amount of time it takes will depend on the number of images in the dataset.</p>

		<h3>Training Your Embedding</h3>
		<p>To begin training your model, navigate to the train tab and fill out the necessary fields as follows:</p>
		<p><tag>Embedding:</tag> Select your placeholder embedding from the drop-down list in the embeddings folder. If it does not appear, try reloading the webGUI.</p>
		<p><tag>Learning rate:</tag> This field allows you to specify the intensity with which the AI will attempt to change the embedding data. It is important to keep this value as low as possible, as neural networks are sensitive to changes in the data. A good rule of thumb is to stay below 0.1 and use the default value of 0.005 if you are unsure. Keep in mind that a low learning rate will result in slower changes to the embedding data, requiring more steps to reach the desired result. However, a lower learning rate will often result in more accurate results.</p>
		<p>You can also specify multiple learning rates to use throughout the training process. Simply use the format <tag>[learning rate]:[step count]</tag> and separate values with commas. For example, <tag>0.005:500, 0.001:1000, 0.004</tag> will train at a learning rate of 0.005 for 500 steps, then switch to 0.001 for an additional 1000 steps, and finally use 0.004 for any remaining steps until training is complete. This can be useful for fine-tuning your training and maximizing the efficiency of your training time.</p>
		<p>Note: if you see <tag>Loss: nan</tag> in the console while training, your learning rate was too high and your embedding is effectively "dead". In this case, you will need to start the training process over again.</p>
		<p><tag>Dataset directory</tag> This is the directory where your dataset is stored. It should be the same as the <tag>Destination directory</tag> set in the preprocessing stage.</p>
		<p><tag>Log directory</tag> This is where the program will save test images. It is recommended to leave the default setting and access the images in the <tag>textual_inversion</tag> folder of the webGUI directory.</p>
		<p><tag>Prompt template file</tag> This is the path to a file containing prompts for the AI training. It is best to use the default setting of <tag>[webGUI directory]\style_filewords.txt</tag> for style training and <tag>subject_filewords.txt</tag> for character training. If you are unsure, it is recommended to not modify the prompt file.</p>
		<p>Advanced users may choose to create their own prompt file or edit the existing ones, but it is recommended to avoid this for the first training session.</p>
		<p><tag>Width & Height</tag> The width and height for the preprocessing stage should be set to 512x512px to maintain consistency.</p>
		<p><tag>Max steps</tag> The max steps field determines the duration of the AI's training. Please note that training may take several hours, even on high-end systems. Based on my own experimentation, I have found that a max step count of 100,000 may be necessary for accurate embeddings, as lower step counts have yielded poor results.</p>
		<p><tag>Save an image to log directory every N steps, 0 to disable</tag> This field enables the saving of images at regular intervals, specified by the user. This can be useful for tracking the progress of training. To disable this function, set the value to 0.</p>
		<p><tag>Save a copy of embedding to log directory every N steps, 0 to disable</tag> Similar to the previous field, this option allows for the regular saving of embeddings at set intervals. This can be useful for reverting to an older version of the embedding if the final results are not satisfactory. To disable this function, set the value to 0.</p>
		<p><tag>Save images with embedding in PNG chunks</tag> This is a recent feature that creates an image that can also be used as an embedding for the model. Please refer to the main page for more information on this function.</p>
		<p><tag>Read parameters from txt2img tab when making previews</tag> This setting only affects the appearance of preview images, and allows the specification of the prompt that the model will use when displaying the preview image with the embedding.</p>
		<p>Once the appropriate settings have been selected, click "Train embedding" to begin the training process. Please note that this may take several hours. Enjoy your new embedding and space heater.</p>
		<br>
		<p>I am still learning a lot when it comes to textual inversion, but one thing to pay attention to is the loss value that is displayed in the output box underneath the images. The goal of the AI is to try and make the loss value as close to 0 as possible. 0 means the model would theoretically be able to generate all images in the dataset 100% accurately. However, this is impossible in the real world and is not an outcome you would want anyway. But it is important to know about the loss value and understand that it is a good estimate of how well your model is training.</p>
  </article>
         </div>
  </body>
</html>
