<!doctype html>
	 <html lang="en">
	 	<head>


	 	<title>CDcruz| Model Training</title>
	 	<meta charset="UTF-8" />
	 	<meta name="description" content="A thorough and informative guide to Stable Diffusion and Waifu Diffusion.">
	 	<meta name="author" content="Craig D'Cruz">
	 	<meta name="viewport" content="width=device-width, minimal-ui">
	 	<link rel='manifest' href='manifest.json'>

	 	<meta name="theme-color" content="#18BDCB"/>

		<link href="assets/css/stable_diffusion.css" rel="stylesheet">

		<link rel="icon" type="image/x-icon" href="assets/icons/favicon.ico">
		<link rel="shortcut icon" type="image/png" href="assets/icons/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="57x57" href="assets/icons/apple-icon-57x57.png">
		<link rel="apple-touch-icon" sizes="60x60" href="assets/icons/apple-icon-60x60.png">
		<link rel="apple-touch-icon" sizes="72x72" href="assets/icons/apple-icon-72x72.png">
		<link rel="apple-touch-icon" sizes="76x76" href="assets/icons/apple-icon-76x76.png">
		<link rel="apple-touch-icon" sizes="114x114" href="assets/icons/apple-icon-114x114.png">
		<link rel="apple-touch-icon" sizes="120x120" href="assets/icons/apple-icon-120x120.png">
		<link rel="apple-touch-icon" sizes="144x144" href="assets/icons/apple-icon-144x144.png">
		<link rel="apple-touch-icon" sizes="152x152" href="assets/icons/apple-icon-152x152.png">
		<link rel="apple-touch-icon" sizes="180x180" href="assets/icons/apple-icon-180x180.png">
		<link rel="icon" type="image/png" sizes="192x192"  href="assets/icons/android-icon-192x192.png">
		<link rel="icon" type="image/png" sizes="32x32" href="assets/icons/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="96x96" href="assets/icons/favicon-96x96.png">
		<link rel="icon" type="image/png" sizes="16x16" href="assets/icons/favicon-16x16.png">
		<meta name="msapplication-TileColor" content="#18BDCB">
		<meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">

		<script src="assets/js/copy.js"></script>
		<script src="assets/js/hsection.js"></script>
	 </head>


  <body>




<div class='wrapper'>

    <h1>Model Training</h1>
    <h2>Overview</h2>
    <hr>
    <article>
		<a href="index.html">Back to the main page</a>
		<p>This is an installation guide for how to install the AUTOMATIC1111 WebGUI (mainly as a reminder for my future self). I have tested installing on Windows 10 & 11 and I plan to try to install on my Macbook as well to provide a guide on that. Most of this information has been taken from Rentry.org but I have added information or clarity were I feel appropriate.</p>
		
		<section>
            <h3 onclick="toggleHsection(this);" style="cursor:pointer; display: flex;">Preparing Data <img loading="lazy" src="assets/images/icons/down_caret.svg" alt="down_caret" style="width: 20px; margin-left:25px;"/></h3>
            <hsection>
              <p>To ensure high-quality data for model training, it is essential to collect a diverse range of images that meet certain criteria. These images should be 512x512 pixels in size and should not contain any text. They should also be free from compression artifacts or blurriness, and ideally only feature a single subject or character. By providing a diverse range of images with a variety of lighting, poses, and expressions, the model will be able to perform well in a wide range of situations.</p>
              <p>One effective method for collecting a large number of high-quality images is to use the Pintrest platform. The Pintrest app allows users to create "boards" where they can save and organize images, and the platform uses machine learning to suggest similar images based on users' interests. However, it is important to note that Pintrest may flag accounts as spam if users pin or like too many images at once. In most cases, the spam block typically lasts less than 24 hours.</p>
              <p><tag>Efficiently download images from Pintrest</tag> Pintrest does not provide a native method for downloading a large number of images at once. However, I have found a solution with the Google Chrome extension "PinDown." This easy-to-use extension can be found on the <a href="https://chrome.google.com/webstore/detail/pindown/flieckppkcgagklbnnhnkkeladdghogp/related" target="_blank" rel="noreferrer">PinDown web store page</a>, and is free to use.</p>
              <p>I have experienced some issues with PinDown not downloading images at all. This may be due to spam blocking implemented by Pintrest. To resolve this, allow the PinDown extension to be used in incognito tabs and open a new incognito tab. Log into Pintrest and go to your desired board. PinDown should now function properly. You can verify that PinDown is working if it is displaing some images from your board in the PinDown banner.</p>
              <p>If you continue to experience problems, try uninstalling and reinstalling the Pindown extension, as this has been known to resolve the issue.</p>
              <p><tag>Bulk smart crop images</tag> (You may skip this step now as the WebGUI supports auto cropping natively, however it is still a good program to use) The website <a href="https://bulkimagecrop.com/" target="_blank" rel="noreferrer">https://bulkimagecrop.com/</a> is an effective tool for quickly cropping images to a 1:1 aspect ratio with a focus on the face. It is free to use once, after which a $10 perpetual license is required. The site runs locally on your computer, eliminating the need for cloud processing. To use the website, select the "Target Aspect Ratio" option and set it to 1:1, then click "Auto Crop".</p>
              <p>The website also allows for previewing and adjusting the crop box for individual images, ensuring that each image is perfectly cropped. However, it can take a significant amount of time to process a large number of images, and the inclusion of invalid file types will cause an error and require restarting the process. It is recommended to use PNG and JPEG/JPG images to avoid such errors.</p>
              <p>Once the cropping is complete, the website will download a zip folder containing all the cropped images, ready for training. When extracting the zip file, make sure to do so in an empty folder as the zip file does not contain subfolders. The results may vary, but the website generally does a good job of keeping the face of the subject in frame. While not perfect, it is much faster than manually cropping hundreds of images with different sizes.</p>
              <p><tag>Bulk Resize</tag> (You may skip this step as the WebGUI does this natively) If you wish to resize your images to 512x512 using a program other than the in-built resizer of the WebGUI, I recommend using <a href="https://bulkresizephotos.com" target="_blank" rel="noreferrer">https://bulkresizephotos.com</a>, which offers similar functionality to BulkImageCrop but is not developed by the same creator. This tool allows for efficient resizing of images and provides the option to download the resized images as a single zip folder.</p>
              <p><tag>Start Training</tag> After ensuring that all your images are in the correct dimensions, you can proceed with training using any of the available methods.</p>
            </hsection>
        
            <h3 onclick="toggleHsection(this);" style="cursor:pointer; display: flex;">Aesthetic Gradients <img loading="lazy" src="assets/images/icons/down_caret.svg" alt="down_caret" style="width: 20px; margin-left:25px;"/></h3>
            <hsection>
              <p>On 22/10/22, Aesthetic Gradients were added as a new feature for the webGUI. While there isn't much information available on how Aesthetics Gradients impact a model, from what I have gathered, they provide a new way to add style to a model without directly modifying the model and offer greater control over the resulting image. If you're using the webGUI, you will need to add the Aesthetic Embedding separately as it is considered an "extension" which was recently added on 24/10/22 to allow for the modular expansion of the GUI.</p>
              <p>Aesthetic Embeddings function as an image prompt for the AI instead of a traditional text prompt. The AI uses the image prompt as a basis for the generated image, providing it with a clearer understanding of what you want the image to look like compared to simply describing it through text. </p>
              <p>Aesthetic Embeddings should not be confused with Textual Inversion ones, and therefore require a separate Aesthetic Embedding to be created in the training tab of the webGUI. Additionally, only one aesthetic embedding can be used at a time, while multiple textual inversion embeddings can be utilized simultaneously. However, aesthetic embeddings only require a few images for training in order to greatly influence the AI.</p>
            <p>This <a href="https://www.youtube.com/watch?v=9zYzuKaYfJw" target="_blank" rel="noreferrer">Youtube video</a> by koiboi does a great job of explaining Aesthetic Embeddings and other basics of Stable Diffusion in a thorough, information way.</p>
            </hsection>
        
            <h3 onclick="toggleHsection(this);" style="cursor:pointer; display: flex;">Textual Inversion <img loading="lazy" src="assets/images/icons/down_caret.svg" alt="down_caret" style="width: 20px; margin-left:25px;"/></h3>
            <hsection>
              <p>Textual Inversion is a method used to add new words or phrases to a model by associating them with a collection of images. It is recommended to train words that the model already has some familiarity with. For example, training the model to recognize a new anime character would work well because the model already knows how to draw people, you are simply adding a specific person to the dataset. Textual Inversion is effective for expanding upon what the model already does and adding new objects or characters.</p>
              <p>Files used for Textual Inversion are called Embeddings, and have the file extensions <tag>.pt</tag> or <tag>.bin</tag>. To use them with the webGUI, simply place the .pt files into the embeddings folder, reload the webGUI, and you are ready to go. Use the custom prompt in your prompts list.</p>
              <p>An update by AUTOMATIC1111 on (16/10/22) added the ability to use special images as embeddings. These images can be in formats such as .png and .webp, and display the result of the embedding as the image along with the prompt and data needed for the embedding to work. Simply place the image in the embeddings folder like any other embedding.</p>
              <p>Embeddings can greatly impact the output of your images, so it may be helpful to reduce their influence using square brackets or another method. They can also be used as negative prompts, which is a useful technique to prevent certain glitches from occurring.</p>
              <p>One advantage of Textual Inversion compared to other training methods is that it requires the least VRAM (8GB minimum) and produces the smallest file size (2-30kb). However, it is also the lowest performing training method, but can still produce usable results.</p>
              <p>Keep in mind that embeddings work best with the model they were trained with. If you switch models and try to use the same embedding, the results may not be as cohesive.</p>
              <p>I have not personally trained any embeddings, but from what I have seen online, the process can be straightforward and produce good results even with fewer than 20 images used for training.</p>
                            
            <p>There is a more detailed explaination on how Texture Inversion works within AUTOMATIC1111 webGUI at the link <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion" target="_blank" rel="noreferrer">AUTOMATIC1111 Textual Inversion</a></p>
            <p>If you have created your own embeddings, please feel free to share it with me so I can add a link/ download on this site for others to use.</p>
            <p>For my guide on how to use Textual Inversion go to my <a href="https://stablediffusion.cdcruz.com/embeddings_training.html">Embeddings Training page</a>.</p>
            </hsection>

            <h3 onclick="toggleHsection(this);" style="cursor:pointer; display: flex;">Hypernetworks <img loading="lazy" src="assets/images/icons/down_caret.svg" alt="down_caret" style="width: 20px; margin-left:25px;"/></h3>
            <hsection>
              <p>Hypernetworks, a relatively new concept, have limited documentation available. From what is known, Hypernets inject a chosen prompt into the final stages of a model's processing flow. In simpler terms, the Hypernet prompt is added during the final steps of rendering an image, which influences the output more than if it were introduced at the beginning with other prompts. For example, the prompt would have less influence if used at the start like Textual Inversion does.</p>
              <p>To train a Hypernet, a graphics card with at least 8GB of VRAM is required.</p>
              <p>Credit goes to Reddit user u/vic8760 for their comments on this topic.</p>
              <br>
              <p>After experimenting with Hypernetworks and Textual Inversion, it appears that Hypernetworks are better at refining pre-existing models to create more visually appealing art. In contrast, Textual Inversion seems to be more effective at inserting unique objects, characters, and concepts into a model due to its modularity. However, I have not been able to train a good embedding yet.</p>
              <p>Hypernetworks train faster than embeddings and improve the model significantly if a good dataset is used for training. Ultimately, the success of all training methods depends on the quality and quantity of the data. The main drawback of Hypernetworks is that only one can be applied to a rendering at a time, which is more limiting than using embeddings. Therefore, it is suggested that Hypernets be used for stylistic and refinement purposes, while embeddings should be utilized for specific objects or concepts that the model should generate. I have not yet had the opportunity to try Dreambooth, but it is believed to be the best of both worlds as it can create standalone models.</p>
              <p>Below is a test of a small anime Hypernetwork I trained with approximately 1.5k unique images and the strength of the Hypernetwork on the X axis. The effect of the Hypernetwork can be subtle or strong. More testing and training is needed before definitive conclusions can be drawn on the effectiveness of Hypernets.</p>
            <img loading="lazy" src="assets/images/examples/hypernetwork_strength_test.webp" alt="An example of hypernetworks with changing strength level." style="max-width: 1200px; width: 100%;"/>
            <br>
            <p>As always renty.org has a great <a href="https://rentry.org/hypernetwork4dumdums" target="_blank" rel="noreferrer">guide on hypernetworks</a>.</p>
            <p>I have found a good Youtube video detailing some of the process for working on Hypernets, although they do not use the AUTOMATIC111 webGUI. <a href="https://www.youtube.com/watch?app=desktop&v=1mEggRgRgfg&feature=share" target="_blank" rel="noreferrer">HYPERNETWORK: Train Stable Diffusion With Your Own Images For FREE!</a></p>
            </hsection>

            <h3 onclick="toggleHsection(this);" style="cursor:pointer; display: flex;">Dreambooth <img loading="lazy" src="assets/images/icons/down_caret.svg" alt="down_caret" style="width: 20px; margin-left:25px;"/></h3>
            <hsection>
              <p>As of 08/11/22, a <a href="https://github.com/d8ahazard/sd_dreambooth_extension" target="_blank" rel="noreferrer">WebGUI extension</a> has been released that allows users to run Dreambooth on low-VRAM/CPU systems within the WebGUI. This is a significant feature that makes custom model creation accessible to a wider audience. However, training the model on lower-end systems may take longer. A basic overview of model training can be found on the GitHub page. Additional information will be added once I have the opportunity to explore Dreambooth further.</p>
              <caution><p>Some users have reported that installing this extension can break the webGUI. I did not experience this issue, but I did have difficulty getting Dreambooth training to work even with CPU processing.</p></caution>
              <p>Dreambooth is based on research by Google and is an improvement on textual inversion. With Dreambooth, users can create their own image models (for a specific character/art style) in a fraction of the time (less than an hour) with higher quality compared to using textual inversion. As Dreambooth creates its own models, they are .ckpt files, which can be cumbersome if you want to have multiple variations. Additionally, Dreambooth model files are much larger as they are considered full models. This is where the modularity of Embeddings shines, as it is more user-friendly to train and generate with.</p>
              <p>Reddit user u/wuduzodemu has created a Dreambooth GUI that offers an easy installation and training interface. I have not yet tested it, but it appears promising and can also run on 10GB VRAM instead of the usual 24GBs required for local Dreambooth training. This is a major improvement and it is hoped that more accessible options like this will continue to be developed for all users. To download the Dreambooth GUI, visit the <a href="https://github.com/smy20011/dreambooth-gui" target="_blank" rel="noreferrer">GitHub page</a>.</p>
            </hsection>
        
            </section>
    </article>
	</body>
</html>
